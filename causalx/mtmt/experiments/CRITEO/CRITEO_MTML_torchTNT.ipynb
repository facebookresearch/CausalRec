{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3675a03a-eef1-42c1-aa56-d64f2c5b8570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from causalml.metrics import auuc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtnt.framework as tnt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from typing import Tuple, Dict\n",
    "import torch\n",
    "from torchtnt.framework import AutoUnit, State\n",
    "from torch import Tensor\n",
    "\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c27836-23eb-4f23-a9b1-01e175ca8ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-layer MLP - Experts\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Expert, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Two-layer MLP - Gates\n",
    "class Gate(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_experts):\n",
    "        super(Gate, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60930196-e335-4d9f-8775-bc722da94f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixture of Experts Model\n",
    "class MixtureOfExperts(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, expert_output_dim, num_experts, num_tasks):\n",
    "        super(MixtureOfExperts, self).__init__()\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(input_dim, hidden_dim, expert_output_dim) for _ in range(num_experts)\n",
    "        ])\n",
    "        self.gates = nn.ModuleDict({\n",
    "            f\"task_{task}_treatment_{treatment}\": Gate(input_dim, hidden_dim, num_experts)\n",
    "            for task in range(num_tasks) for treatment in range(2)\n",
    "        })\n",
    "        self.task_heads = nn.ModuleDict({\n",
    "            f\"task_{task}_treatment_{treatment}\": nn.Linear(expert_output_dim, 1)\n",
    "            for task in range(num_tasks) for treatment in range(2)\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        expert_outputs = torch.stack(\n",
    "            [expert(x) for expert in self.experts], dim=-1\n",
    "        )  # Shape: (batch_size, expert_output_dim, num_experts)\n",
    "        outputs = {}\n",
    "\n",
    "        for key, gate in self.gates.items():\n",
    "            gate_weights = gate(x)  # Shape: (batch_size, num_experts)\n",
    "            gate_weights = gate_weights.unsqueeze(1)  # Shape: (batch_size, 1, num_experts)\n",
    "            mixture_output = torch.bmm(\n",
    "                expert_outputs, gate_weights.transpose(1, 2)\n",
    "            ).squeeze(2)  # Shape: (batch_size, expert_output_dim)\n",
    "            task_output = self.task_heads[key](mixture_output)  # Shape: (batch_size, 1)\n",
    "            outputs[key] = task_output\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1125b345-60af-48ad-85a1-f13a0d3cd018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preprocessing\n",
    "criteo_data_path = \"criteo_uplift_v2.1.csv\"\n",
    "data = pd.read_csv(criteo_data_path)\n",
    "\n",
    "X = data.drop(columns=[\"treatment\", \"visit\", \"conversion\", \"exposure\"]).values\n",
    "A = data[\"treatment\"].values\n",
    "Y = data[\"visit\"].values\n",
    "C = data[\"conversion\"].values\n",
    "\n",
    "# Train-test split\n",
    "N = len(X)\n",
    "indices = np.random.permutation(N)\n",
    "split_idx = int(N * 0.8)\n",
    "train_indices, test_indices = indices[:split_idx], indices[split_idx:]\n",
    "\n",
    "X_train = X[train_indices]\n",
    "X_test = X[test_indices]\n",
    "A_train = A[train_indices]\n",
    "A_test = A[test_indices]\n",
    "Y_train = Y[train_indices]\n",
    "Y_test = Y[test_indices]\n",
    "C_train = C[train_indices]\n",
    "C_test = C[test_indices]\n",
    "\n",
    "# Normalize X to (0, 1) range using training data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)  # Fit on training data and transform it\n",
    "X_test_normalized = scaler.transform(X_test)        # Apply the same transformation to test data\n",
    "\n",
    "# Convert to tensors and move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_normalized, dtype=torch.float32, device=device)\n",
    "X_test_tensor = torch.tensor(X_test_normalized, dtype=torch.float32, device=device)\n",
    "A_train_tensor = torch.tensor(A_train, dtype=torch.float32, device=device)\n",
    "A_test_tensor = torch.tensor(A_test, dtype=torch.float32, device=device)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32, device=device)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32, device=device)\n",
    "C_train_tensor = torch.tensor(C_train, dtype=torch.float32, device=device)\n",
    "C_test_tensor = torch.tensor(C_test, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50646301-7220-45e0-a0fb-5e9febdc7c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class UpliftTrainingUnit(AutoUnit):\n",
    "    def __init__(self, model, loss_fn):\n",
    "        super().__init__(module=model)\n",
    "        self.loss_fn = loss_fn\n",
    "        \n",
    "    def forward(self, data: Tuple[Tensor, Tensor, Tensor, Tensor]) -> Dict[str, Tensor]:\n",
    "        X_batch, A_batch, Y_batch, C_batch = data\n",
    "        outputs = self.module(X_batch)\n",
    "        return outputs\n",
    "    \n",
    "    def compute_loss(self, state: State, data: Tuple[Tensor, Tensor, Tensor, Tensor]) -> Tensor:\n",
    "        X_batch, A_batch, Y_batch, C_batch = data\n",
    "        outputs = self.forward(data)\n",
    "        batch_losses = []\n",
    "        \n",
    "        # Process each sample in the batch\n",
    "        for i in range(len(X_batch)):\n",
    "            sample_losses = []\n",
    "            \n",
    "            # Get individual sample\n",
    "            treatment = A_batch[i].item()\n",
    "            \n",
    "            if treatment == 0:\n",
    "                # Treatment 0 predictions\n",
    "                y_pred = outputs[\"task_0_treatment_0\"][i:i+1]\n",
    "                y_true = Y_batch[i:i+1].unsqueeze(1)\n",
    "                sample_losses.append(self.loss_fn(y_pred, y_true))\n",
    "                \n",
    "                c_pred = outputs[\"task_1_treatment_0\"][i:i+1]\n",
    "                c_true = C_batch[i:i+1].unsqueeze(1)\n",
    "                sample_losses.append(self.loss_fn(c_pred, c_true))\n",
    "                \n",
    "            else:\n",
    "                # Treatment 1 predictions\n",
    "                y_pred = outputs[\"task_0_treatment_1\"][i:i+1]\n",
    "                y_true = Y_batch[i:i+1].unsqueeze(1)\n",
    "                sample_losses.append(self.loss_fn(y_pred, y_true))\n",
    "                \n",
    "                c_pred = outputs[\"task_1_treatment_1\"][i:i+1]\n",
    "                c_true = C_batch[i:i+1].unsqueeze(1)\n",
    "                sample_losses.append(self.loss_fn(c_pred, c_true))\n",
    "            \n",
    "            # Combine losses for this sample\n",
    "            if sample_losses:\n",
    "                sample_loss = torch.stack(sample_losses).mean()\n",
    "                batch_losses.append(sample_loss)\n",
    "        \n",
    "        # Combine all batch losses\n",
    "        if batch_losses:\n",
    "            total_loss = torch.stack(batch_losses).mean()\n",
    "            state.loss = total_loss  # Store loss in state\n",
    "            return total_loss\n",
    "        else:\n",
    "            # Return zero loss if no valid samples\n",
    "            return torch.tensor(0.0, device=X_batch.device, requires_grad=True)\n",
    "\n",
    "    def train_step(self, state: State, data: Tuple[Tensor, Tensor, Tensor, Tensor]) -> State:\n",
    "        \"\"\"Training step for the model.\"\"\"\n",
    "        # Forward pass and compute loss\n",
    "        loss = self.compute_loss(state, data)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        return state\n",
    "        \n",
    "    def eval_step(self, state: State, data: Tuple[Tensor, Tensor, Tensor, Tensor]) -> State:\n",
    "        \"\"\"Evaluation step for the model.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            loss = self.compute_loss(state, data)\n",
    "            state.loss = loss\n",
    "        return state\n",
    "\n",
    "    def configure_optimizers_and_lr_scheduler(self, module):\n",
    "        optimizer = torch.optim.Adam(module.parameters(), lr=0.001)\n",
    "        return optimizer, None\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "batch_size = 10000\n",
    "train_dataset = TensorDataset(X_train_tensor, A_train_tensor, Y_train_tensor, C_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, A_test_tensor, Y_test_tensor, C_test_tensor)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize model and training\n",
    "input_dim = X.shape[1]\n",
    "hidden_dim = 16\n",
    "expert_output_dim = 16\n",
    "num_experts = 10\n",
    "num_tasks = 2\n",
    "\n",
    "model = MixtureOfExperts(input_dim, hidden_dim, expert_output_dim, num_experts, num_tasks).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Create training unit and run training\n",
    "unit = UpliftTrainingUnit(model=model, loss_fn=loss_fn)\n",
    "\n",
    "# Training loop with TorchTNT\n",
    "tnt.fit(\n",
    "    unit=unit,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    max_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d674c7fb-97ec-430a-a0a5-1be4dcb1862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_uplift_model(\n",
    "    model: torch.nn.Module,\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    device: torch.device\n",
    ") -> Tuple[Dict[str, float], Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Evaluate uplift model and calculate AUUC scores for visit and conversion.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained uplift model\n",
    "        test_dataloader: DataLoader containing test data\n",
    "        device: Device to run the model on\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing metrics dictionary and evaluation dataframes\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        uplift_scores_visit = []\n",
    "        uplift_scores_conversion = []\n",
    "        A_test_list = []\n",
    "        Y_test_list = []\n",
    "        C_test_list = []\n",
    "        \n",
    "        # Collect predictions\n",
    "        for X_batch, A_batch, Y_batch, C_batch in test_dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            \n",
    "            # Get predictions for both treatments\n",
    "            visit_preds_0 = outputs[\"task_0_treatment_0\"].squeeze()\n",
    "            visit_preds_1 = outputs[\"task_0_treatment_1\"].squeeze()\n",
    "            conversion_preds_0 = outputs[\"task_1_treatment_0\"].squeeze()\n",
    "            conversion_preds_1 = outputs[\"task_1_treatment_1\"].squeeze()\n",
    "            \n",
    "            # Calculate uplift scores\n",
    "            uplift_visit = (visit_preds_1 - visit_preds_0).cpu()\n",
    "            uplift_conversion = (conversion_preds_1 - conversion_preds_0).cpu()\n",
    "            \n",
    "            # Collect results\n",
    "            uplift_scores_visit.append(uplift_visit)\n",
    "            uplift_scores_conversion.append(uplift_conversion)\n",
    "            A_test_list.append(A_batch.cpu())\n",
    "            Y_test_list.append(Y_batch.cpu())\n",
    "            C_test_list.append(C_batch.cpu())\n",
    "        \n",
    "        # Concatenate all collected data\n",
    "        uplift_scores_visit = torch.cat(uplift_scores_visit).numpy()\n",
    "        uplift_scores_conversion = torch.cat(uplift_scores_conversion).numpy()\n",
    "        A_test_all = torch.cat(A_test_list).numpy()\n",
    "        Y_test_all = torch.cat(Y_test_list).numpy()\n",
    "        C_test_all = torch.cat(C_test_list).numpy()\n",
    "        \n",
    "        # Create evaluation DataFrames\n",
    "        df_visit = pd.DataFrame({\n",
    "            'treatment': A_test_all,\n",
    "            'outcome': Y_test_all,\n",
    "            'uplift_score': uplift_scores_visit\n",
    "        })\n",
    "        \n",
    "        df_conversion = pd.DataFrame({\n",
    "            'treatment': A_test_all,\n",
    "            'outcome': C_test_all,\n",
    "            'uplift_score': uplift_scores_conversion\n",
    "        })\n",
    "        \n",
    "        # Calculate AUUC scores\n",
    "        auuc_visit = auuc_score(\n",
    "            df_visit, \n",
    "            outcome_col='outcome',\n",
    "            treatment_col='treatment',\n",
    "            score_col='uplift_score',\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        auuc_conversion = auuc_score(\n",
    "            df_conversion,\n",
    "            outcome_col='outcome',\n",
    "            treatment_col='treatment',\n",
    "            score_col='uplift_score',\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        treatment_effect_visit = np.mean(Y_test_all[A_test_all == 1]) - np.mean(Y_test_all[A_test_all == 0])\n",
    "        treatment_effect_conversion = np.mean(C_test_all[A_test_all == 1]) - np.mean(C_test_all[A_test_all == 0])\n",
    "        \n",
    "        # Compile metrics\n",
    "        metrics = {\n",
    "            'auuc_visit': auuc_visit,\n",
    "            'auuc_conversion': auuc_conversion,\n",
    "            'ate_visit': treatment_effect_visit,\n",
    "            'ate_conversion': treatment_effect_conversion\n",
    "        }\n",
    "        \n",
    "        # Store evaluation dataframes\n",
    "        eval_dfs = {\n",
    "            'visit': df_visit,\n",
    "            'conversion': df_conversion\n",
    "        }\n",
    "        \n",
    "        return metrics, eval_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa2f7c0c-27c8-4898-8d78-6c2436814b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uplift Model Evaluation Results:\n",
      "------------------------------\n",
      "AUUC (Visit): 0.7310\n",
      "AUUC (Conversion): 0.8428\n",
      "Average Treatment Effect (Visit): 0.0105\n",
      "Average Treatment Effect (Conversion): 0.0012\n"
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "metrics, eval_dfs = evaluate_uplift_model(model, test_dataloader, device)\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nUplift Model Evaluation Results:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"AUUC (Visit): {metrics['auuc_visit'].iloc[0]:.4f}\")\n",
    "print(f\"AUUC (Conversion): {metrics['auuc_conversion'].iloc[0]:.4f}\")\n",
    "print(f\"Average Treatment Effect (Visit): {metrics['ate_visit']:.4f}\")\n",
    "print(f\"Average Treatment Effect (Conversion): {metrics['ate_conversion']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea2141a-7f74-4783-a479-63e5701290f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
