{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3675a03a-eef1-42c1-aa56-d64f2c5b8570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/yilingliu/miniconda3/envs/pytorch_aws/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from causalml.metrics import auuc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Tuple\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtnt.framework as tnt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from typing import Tuple, Dict\n",
    "import torch\n",
    "from torchtnt.framework import AutoUnit, State\n",
    "from torch import Tensor\n",
    "\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89c27836-23eb-4f23-a9b1-01e175ca8ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-layer MLP - Experts\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Expert, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Two-layer MLP - Gates\n",
    "class Gate(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_experts):\n",
    "        super(Gate, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60930196-e335-4d9f-8775-bc722da94f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixture of Experts Model\n",
    "class MixtureOfExperts(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, expert_output_dim, num_experts, num_tasks):\n",
    "        super(MixtureOfExperts, self).__init__()\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(input_dim, hidden_dim, expert_output_dim) for _ in range(num_experts)\n",
    "        ])\n",
    "        self.gates = nn.ModuleDict({\n",
    "            f\"task_{task}_treatment_{treatment}\": Gate(input_dim, hidden_dim, num_experts)\n",
    "            for task in range(num_tasks) for treatment in range(2)\n",
    "        })\n",
    "        self.task_heads = nn.ModuleDict({\n",
    "            f\"task_{task}_treatment_{treatment}\": nn.Linear(expert_output_dim, 1)\n",
    "            for task in range(num_tasks) for treatment in range(2)\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        expert_outputs = torch.stack(\n",
    "            [expert(x) for expert in self.experts], dim=-1\n",
    "        )  # Shape: (batch_size, expert_output_dim, num_experts)\n",
    "        outputs = {}\n",
    "\n",
    "        for key, gate in self.gates.items():\n",
    "            gate_weights = gate(x)  # Shape: (batch_size, num_experts)\n",
    "            gate_weights = gate_weights.unsqueeze(1)  # Shape: (batch_size, 1, num_experts)\n",
    "            mixture_output = torch.bmm(\n",
    "                expert_outputs, gate_weights.transpose(1, 2)\n",
    "            ).squeeze(2)  # Shape: (batch_size, expert_output_dim)\n",
    "            task_output = self.task_heads[key](mixture_output)  # Shape: (batch_size, 1)\n",
    "            outputs[key] = task_output\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1125b345-60af-48ad-85a1-f13a0d3cd018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "training_path = \"/data/home/yilingliu/MTML/experiments/MT_lift/MT_LIFT/train.csv\"\n",
    "testing_path = \"/data/home/yilingliu/MTML/experiments/MT_lift/MT_LIFT/test.csv\"\n",
    "\n",
    "training_data = pd.read_csv(training_path)\n",
    "testing_data = pd.read_csv(testing_path)\n",
    "\n",
    "# Convert all non-zero treatment values to 1\n",
    "training_data[\"treatment\"] = (training_data[\"treatment\"] != 0).astype(int)\n",
    "testing_data[\"treatment\"] = (testing_data[\"treatment\"] != 0).astype(int)\n",
    "\n",
    "# Preprocess data\n",
    "X_train = training_data.drop(columns=[\"click\", \"conversion\", \"treatment\"]).values\n",
    "X_test = testing_data.drop(columns=[\"click\", \"conversion\", \"treatment\"]).values\n",
    "A_train = training_data[\"treatment\"].values\n",
    "A_test = testing_data[\"treatment\"].values\n",
    "Y_train = training_data[\"click\"].values\n",
    "Y_test = testing_data[\"click\"].values\n",
    "C_train = training_data[\"conversion\"].values\n",
    "C_test = testing_data[\"conversion\"].values\n",
    "\n",
    "# Normalize X to (0, 1) range using training data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)  # Fit on training data and transform it\n",
    "X_test_normalized = scaler.transform(X_test)        # Apply the same transformation to test data\n",
    "\n",
    "# Convert to tensors and move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_normalized, dtype=torch.float32, device=device)\n",
    "X_test_tensor = torch.tensor(X_test_normalized, dtype=torch.float32, device=device)\n",
    "A_train_tensor = torch.tensor(A_train, dtype=torch.float32, device=device)\n",
    "A_test_tensor = torch.tensor(A_test, dtype=torch.float32, device=device)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32, device=device)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32, device=device)\n",
    "C_train_tensor = torch.tensor(C_train, dtype=torch.float32, device=device)\n",
    "C_test_tensor = torch.tensor(C_test, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50646301-7220-45e0-a0fb-5e9febdc7c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7242, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7236, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7232, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7228, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7223, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7219, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7214, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7211, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7207, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7204, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7200, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7195, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7191, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7184, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7180, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7178, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7173, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7169, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7165, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7161, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7155, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7152, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7148, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7142, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7139, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7134, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7128, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7125, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7120, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7116, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7110, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7108, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7104, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7099, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7094, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7090, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7085, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7074, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7071, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7066, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7057, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7053, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7048, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.7043, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class UpliftTrainingUnit(AutoUnit):\n",
    "    def __init__(self, model, loss_fn):\n",
    "        super().__init__(module=model)\n",
    "        self.loss_fn = loss_fn\n",
    "        \n",
    "    def forward(self, data: Tuple[Tensor, Tensor, Tensor, Tensor]) -> Dict[str, Tensor]:\n",
    "        X_batch, A_batch, Y_batch, C_batch = data\n",
    "        outputs = self.module(X_batch)\n",
    "        return outputs\n",
    "    \n",
    "    def compute_loss(self, state: State, data: Tuple[Tensor, Tensor, Tensor, Tensor]) -> Tensor:\n",
    "        X_batch, A_batch, Y_batch, C_batch = data\n",
    "        outputs = self.forward(data)\n",
    "    \n",
    "        # Create masks for treatments\n",
    "        treatment_mask = (A_batch == 1)\n",
    "        control_mask = (A_batch == 0)\n",
    "    \n",
    "        # Extract predictions for all samples based on treatment\n",
    "        y_pred_treated = outputs[\"task_0_treatment_1\"][treatment_mask]\n",
    "        y_true_treated = Y_batch[treatment_mask]\n",
    "    \n",
    "        y_pred_control = outputs[\"task_0_treatment_0\"][control_mask]\n",
    "        y_true_control = Y_batch[control_mask]\n",
    "    \n",
    "        c_pred_treated = outputs[\"task_1_treatment_1\"][treatment_mask]\n",
    "        c_true_treated = C_batch[treatment_mask]\n",
    "    \n",
    "        c_pred_control = outputs[\"task_1_treatment_0\"][control_mask]\n",
    "        c_true_control = C_batch[control_mask]\n",
    "    \n",
    "        # Compute losses in a batch-wise manner\n",
    "        y_loss_treated = self.loss_fn(y_pred_treated, y_true_treated.unsqueeze(1))\n",
    "        y_loss_control = self.loss_fn(y_pred_control, y_true_control.unsqueeze(1))\n",
    "        \n",
    "        c_loss_treated = self.loss_fn(c_pred_treated, c_true_treated.unsqueeze(1))\n",
    "        c_loss_control = self.loss_fn(c_pred_control, c_true_control.unsqueeze(1))\n",
    "\n",
    "        # Average all sample losses\n",
    "        total_loss = (y_loss_treated + y_loss_control + c_loss_treated + c_loss_control) / 4\n",
    "    \n",
    "        # Store the loss in the state\n",
    "        state.loss = total_loss\n",
    "        return total_loss\n",
    "\n",
    "    def train_step(self, state: State, data: Tuple[Tensor, Tensor, Tensor, Tensor]) -> State:\n",
    "        \"\"\"Training step for the model.\"\"\"\n",
    "        # Forward pass and compute loss\n",
    "        loss = self.compute_loss(state, data)\n",
    "        \n",
    "        # Backward pass and optimizer step\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        print(loss)\n",
    "        return state\n",
    "        \n",
    "    def eval_step(self, state: State, data: Tuple[Tensor, Tensor, Tensor, Tensor]) -> State:\n",
    "        \"\"\"Evaluation step for the model.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            loss = self.compute_loss(state, data)\n",
    "            state.loss = loss\n",
    "        return state\n",
    "\n",
    "    def configure_optimizers_and_lr_scheduler(self, module):\n",
    "        optimizer = torch.optim.Adam(module.parameters(), lr=0.0001)\n",
    "        return optimizer, None\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "batch_size = 100000\n",
    "train_dataset = TensorDataset(X_train_tensor, A_train_tensor, Y_train_tensor, C_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, A_test_tensor, Y_test_tensor, C_test_tensor)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize model and training\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 16\n",
    "expert_output_dim = 16\n",
    "num_experts = 10\n",
    "num_tasks = 2\n",
    "\n",
    "model = MixtureOfExperts(input_dim, hidden_dim, expert_output_dim, num_experts, num_tasks).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Create training unit and run training\n",
    "unit = UpliftTrainingUnit(model=model, loss_fn=loss_fn)\n",
    "\n",
    "# Training loop with TorchTNT\n",
    "tnt.fit(\n",
    "    unit=unit,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=test_dataloader,\n",
    "    max_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d674c7fb-97ec-430a-a0a5-1be4dcb1862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_uplift_model(\n",
    "    model: torch.nn.Module,\n",
    "    test_dataloader: torch.utils.data.DataLoader,\n",
    "    device: torch.device\n",
    ") -> Tuple[Dict[str, float], Dict[str, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Evaluate uplift model and calculate AUUC scores for visit and conversion.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained uplift model\n",
    "        test_dataloader: DataLoader containing test data\n",
    "        device: Device to run the model on\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing metrics dictionary and evaluation dataframes\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        uplift_scores_visit = []\n",
    "        uplift_scores_conversion = []\n",
    "        A_test_list = []\n",
    "        Y_test_list = []\n",
    "        C_test_list = []\n",
    "        \n",
    "        # Collect predictions\n",
    "        for X_batch, A_batch, Y_batch, C_batch in test_dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            \n",
    "            # Get predictions for both treatments\n",
    "            visit_preds_0 = outputs[\"task_0_treatment_0\"].squeeze()\n",
    "            visit_preds_1 = outputs[\"task_0_treatment_1\"].squeeze()\n",
    "            conversion_preds_0 = outputs[\"task_1_treatment_0\"].squeeze()\n",
    "            conversion_preds_1 = outputs[\"task_1_treatment_1\"].squeeze()\n",
    "            \n",
    "            # Calculate uplift scores\n",
    "            uplift_visit = (visit_preds_1 - visit_preds_0).cpu()\n",
    "            uplift_conversion = (conversion_preds_1 - conversion_preds_0).cpu()\n",
    "            \n",
    "            # Collect results\n",
    "            uplift_scores_visit.append(uplift_visit)\n",
    "            uplift_scores_conversion.append(uplift_conversion)\n",
    "            A_test_list.append(A_batch.cpu())\n",
    "            Y_test_list.append(Y_batch.cpu())\n",
    "            C_test_list.append(C_batch.cpu())\n",
    "        \n",
    "        # Concatenate all collected data\n",
    "        uplift_scores_visit = torch.cat(uplift_scores_visit).numpy()\n",
    "        uplift_scores_conversion = torch.cat(uplift_scores_conversion).numpy()\n",
    "        A_test_all = torch.cat(A_test_list).numpy()\n",
    "        Y_test_all = torch.cat(Y_test_list).numpy()\n",
    "        C_test_all = torch.cat(C_test_list).numpy()\n",
    "        \n",
    "        # Create evaluation DataFrames\n",
    "        df_visit = pd.DataFrame({\n",
    "            'treatment': A_test_all,\n",
    "            'outcome': Y_test_all,\n",
    "            'uplift_score': uplift_scores_visit\n",
    "        })\n",
    "        \n",
    "        df_conversion = pd.DataFrame({\n",
    "            'treatment': A_test_all,\n",
    "            'outcome': C_test_all,\n",
    "            'uplift_score': uplift_scores_conversion\n",
    "        })\n",
    "        \n",
    "        # Calculate AUUC scores\n",
    "        auuc_visit = auuc_score(\n",
    "            df_visit, \n",
    "            outcome_col='outcome',\n",
    "            treatment_col='treatment',\n",
    "            score_col='uplift_score',\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        auuc_conversion = auuc_score(\n",
    "            df_conversion,\n",
    "            outcome_col='outcome',\n",
    "            treatment_col='treatment',\n",
    "            score_col='uplift_score',\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        treatment_effect_visit = np.mean(Y_test_all[A_test_all == 1]) - np.mean(Y_test_all[A_test_all == 0])\n",
    "        treatment_effect_conversion = np.mean(C_test_all[A_test_all == 1]) - np.mean(C_test_all[A_test_all == 0])\n",
    "        \n",
    "        # Compile metrics\n",
    "        metrics = {\n",
    "            'auuc_visit': auuc_visit,\n",
    "            'auuc_conversion': auuc_conversion,\n",
    "            'ate_visit': treatment_effect_visit,\n",
    "            'ate_conversion': treatment_effect_conversion\n",
    "        }\n",
    "        \n",
    "        # Store evaluation dataframes\n",
    "        eval_dfs = {\n",
    "            'visit': df_visit,\n",
    "            'conversion': df_conversion\n",
    "        }\n",
    "        \n",
    "        return metrics, eval_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa2f7c0c-27c8-4898-8d78-6c2436814b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uplift Model Evaluation Results:\n",
      "------------------------------\n",
      "AUUC (Visit): 0.5168\n",
      "AUUC (Conversion): 0.5313\n",
      "Average Treatment Effect (Visit): 0.1483\n",
      "Average Treatment Effect (Conversion): 0.0526\n"
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "metrics, eval_dfs = evaluate_uplift_model(model, test_dataloader, device)\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nUplift Model Evaluation Results:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"AUUC (Visit): {metrics['auuc_visit'].iloc[0]:.4f}\")\n",
    "print(f\"AUUC (Conversion): {metrics['auuc_conversion'].iloc[0]:.4f}\")\n",
    "print(f\"Average Treatment Effect (Visit): {metrics['ate_visit']:.4f}\")\n",
    "print(f\"Average Treatment Effect (Conversion): {metrics['ate_conversion']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63ad52a-2343-4cd5-b0ca-1440ff038570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
